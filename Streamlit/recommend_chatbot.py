import streamlit as st 
import pandas as pd
import torch
import os
from PIL import Image
##################################################################api
from dotenv import load_dotenv
import google.generativeai as genai
##################################################################vectordb
import chromadb
from langchain_chroma import Chroma
##################################################################embedding
from transformers import AutoTokenizer, AutoModel
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
##################################################################answer
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import GoogleGenerativeAI

load_dotenv()

gemini_api_key = os.getenv('GEMINI_API_KEY')
genai.configure(api_key=gemini_api_key)

##################################################################
#chatbot UI
st.set_page_config(page_title='ì œì£¼ë„ ë§›ì§‘', page_icon="ğŸ†",initial_sidebar_state="expanded")
st.title('ì œì£¼ë„ ìŒì‹ì  íƒë°©!')
st.subheader("ëˆ„êµ¬ë‘ ì œì£¼ë„ ì™”ë‚˜ìš”? ë§ì¶¤ ì œì£¼ë„ ë§›ì§‘ ì¶”ì²œí•´ë“œë ¤ìš”~")

st.write("")

st.write("#ì—°ì¸#ì•„ì´#ì¹œêµ¬#ë¶€ëª¨ë‹˜#í˜¼ì#ë°˜ë ¤ë™ë¬¼ #ë°ì´íŠ¸#ë‚˜ë“¤ì´#ì—¬í–‰#ì¼ìƒ#íšŒì‹#ê¸°ë…ì¼...")
st.write("")

with st.sidebar:
    st.title('<ì˜µì…˜ì„ ì„ íƒí•˜ë©´ ë¹ ë¥´ê²Œ ì¶”ì²œí•´ë“œë ¤ìš”!>')
    st.write("")
    
    st.subheader('ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”! í•´ë‹¹ ì§€ì—­ì˜ ë§›ì§‘ì„ ì°¾ì•„ë“œë¦´ê»˜ìš”.')
    st.write("")
    
    # ì²´í¬ë°•ìŠ¤ ì‚¬ìš©
    local_jeju_city = st.checkbox('ì œì£¼ì‹œ')  # ì œì£¼ì‹œ ì²´í¬ë°•ìŠ¤
    local_seogwipo_city = st.checkbox('ì„œê·€í¬ì‹œ')  # ì„œê·€í¬ì‹œ ì²´í¬ë°•ìŠ¤
    st.write("")
    
    # PNG ì´ë¯¸ì§€ ì‚½ì…
    image = Image.open(r'D:\2024_bigcontest\data\ì´ë¯¸ì§€\ì œì£¼ë„ ì§€ë„.png')  # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
    st.image(image, caption='ì œì£¼ë„ ì§€ë„', use_column_width=True)  # ì‚¬ì´ë“œë°”ì— ì´ë¯¸ì§€ ì‚½ì…

# Store LLM generated responses
if "messages" not in st.session_state:
    st.session_state.messages = []

# Check if the initial assistant message has been displayed
if "message_displayed" not in st.session_state:
    st.session_state.message_displayed = False

# Display the initial assistant message only once
if not st.session_state.message_displayed:
    st.session_state.messages.append({"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ì„ ì°¾ìœ¼ì‹œë‚˜ìš”?"})
    st.session_state.message_displayed = True  # Mark message as displayed

# Display previous messages if they exist
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.write(message["content"])
        
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ì„ ì°¾ìœ¼ì‹œë‚˜ìš”?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)
  
##################################################################    
#llm í•¨ìˆ˜ 
def get_llm():
    generation_config = {
        "temperature": 0,
        "top_p": 0.95,
        "top_k": 64,
        "max_output_tokens": 8192,
        "response_mime_type": "text/plain",
    }

    # GoogleGenerativeAI ëª¨ë¸ ì´ˆê¸°í™”
    llm = GoogleGenerativeAI(
        model="gemini-1.5-flash",   # ëª¨ë¸ ì´ë¦„ ì„¤ì •
        api_key=gemini_api_key,  # í•„ìˆ˜ ì…ë ¥ í•„ë“œ
        **generation_config          # ì¶”ê°€ ì„¤ì •ê°’ ì „ë‹¬
    )
    return llm

#########################ì„ë² ë”© ëª¨ë¸ ë¡œë“œ##############################    
@st.cache_resource
def load_embedding_model():
    model_name = "jhgan/ko-sroberta-multitask"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    embedding_function = HuggingFaceEmbeddings(model_name=model_name)
    return tokenizer, model, embedding_function

tokenizer, model, embedding_function = load_embedding_model()

#########################ì„ë² ë”© í•¨ìˆ˜##############################    
# í…ìŠ¤íŠ¸ ì„ë² ë”©
def embed_text(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        embeddings = model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()

###########################ChromaDB#############################
# ChromaDB ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì • (ì´ì „ì— ì €ì¥í–ˆë˜ ê²½ë¡œ)
embedding_function = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')

# ChromaDB ë¶ˆëŸ¬ì˜¤ê¸°
db_folder = r'D:\2024_bigcontest\VectorDB\mct_keyword_v5'
client = chromadb.PersistentClient(path=db_folder)
collection = client.get_collection("jeju_store_mct_keyword_5")

# search_retriever = search_store.as_retriever(search_kwargs={'k' : 10})
###########################retreiver##########################
# ìŒì‹ì  ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def search_restaurants(query):
    query_embedding = embed_text(query)  # ì§ˆë¬¸ ì„ë² ë”©
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=5  # ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
    )
    # 'ì œì£¼ì‹œ' ì£¼ì†Œê°€ í¬í•¨ëœ ë©”íƒ€ë°ì´í„° í•„í„°ë§
    filtered_results = []
    if local_jeju_city:
        for metadata in results['metadatas'][0]:
            if 'ì œì£¼ì‹œ' in metadata.get('address', ''):  # 'address'ì— 'ì œì£¼ì‹œ'ê°€ í¬í•¨ëœ ë°ì´í„° 
                filtered_results.append(metadata)
    elif local_seogwipo_city:
        for metadata in results['metadatas'][0]:
            if 'ì„œê·€í¬ì‹œ' in metadata.get('address', ''):  # 'address'ì— 'ì„œê·€í¬ì‹œ'ê°€ í¬í•¨ëœ ë°ì´í„° 
                filtered_results.append(metadata)
    else:
        for metadata in results['metadatas'][0]: # ì œì£¼ì‹œ & ì„œê·€í¬ì‹œê°€ í¬í•¨ëœ ë°ì´í„° 
            filtered_results.append(metadata)

    return filtered_results

##################################################################
# ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def generate_response(user_input):
    # ê° ë¬¸ì„œì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ìš”ì•½ë¬¸ ìƒì„±
    search_results = search_restaurants(user_input) 
    responses = []
    for doc in search_results:  # ìƒìœ„ 3ê°œì˜ ë¬¸ì„œë§Œ ì‚¬ìš©
        context = {
        "name": doc['name'], #ê°€ê²Œëª…
        "address": doc['address'], #ì£¼ì†Œ
        "industry": doc['industry'], #ì—…ì¢…
        "attraction": ', '.join(doc['attraction'][:3]), #ì£¼ë³€ ê´€ê´‘ì§€
        "summary": doc['summary'] # ìš”ì•½ 
    } 
    
# LLMì—ê²Œ ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ìš”ì²­
    prompt_template  = """
    ë‹¤ìŒì˜ ê°€ê²Œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ê°€ê²Œì— ëŒ€í•œ ì¶”ì²œ ë‚´ìš©ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”:
    ê°€ê²Œ ì •ë³´ :
    name: {name}
    address: {address}
    industry: {industry}
    attraction: {attraction}
    summary: {summary}
    ---
    ë‹¤ìŒì˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ë˜ëŠ” ê²°ê³¼ë¬¼(ê°€ê²Œ ì •ë³´) ë°˜ë“œì‹œ ë‹¤ìŒ [ì˜ˆì‹œ]ê³¼ ê°™ì´ ì¶œë ¥ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    ---
    [ì˜ˆì‹œ]
    ê°€ê²Œ : ì„œì€ì´ë„¤ìƒê³ ê¸° \n
    ì£¼ì†Œ : ì œì£¼ ì œì£¼ì‹œ ì—°ë™ 274-30ë²ˆì§€ 1ì¸µ \n
    ì—…ì¢… : ìœ¡ë¥˜,ê³ ê¸°ìš”ë¦¬ \n
    ì£¼ë³€ ê´€ê´‘ì§€ : ì— ë²„í˜¸í…”, ìˆ˜ëª©ì›í…Œë§ˆíŒŒí¬, ë¡¯ë°ì‹œí‹°í˜¸í…” ì œì£¼ \n
    ê°€ê²Œ íŠ¹ì§• : (ì§ˆë¬¸ì— ëŒ€í•œ ìš”ì•½ìœ¼ë¡œ ë³´ì—¬ì£¼ê¸°) \n
    ---
    
    ìš”ì•½ì˜ ê²½ìš°, queryì— ëŒ€í•œ ìš”ì•½ìœ¼ë¡œ ë³´ì—¬ì£¼ì„¸ìš”.
    ë§Œì¼, ìœ„ì˜ í˜•íƒœë¡œ ë‹µë³€ì´ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ê²°ê³¼ë¬¼ì€ ë©”ì‹œì§€ë¥¼ ë³´ì—¬ì£¼ì§€ ë§ˆì„¸ìš”.
    ë¶ˆí•„ìš”í•œ ê¸°í˜¸ëŠ” ì—†ì• ê³  ì‘ì„±í•´ì£¼ì„¸ìš”.
    """

    # í”„ë¡¬í”„íŠ¸ì— ë°ì´í„° ì±„ìš°ê¸°
    prompt = prompt_template.format(**context)
    
    # LLMì„ ì´ìš©í•´ ì‘ë‹µ ìƒì„±
    llm = get_llm()  # ì‚¬ìš© ì¤‘ì¸ LLMì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    response = llm(prompt)
    responses.append(response)

    # ëª¨ë“  ì œí’ˆì— ëŒ€í•œ ì¶”ì²œ ë¬¸ì¥ì„ ê²°í•©í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±
    final_response = response
    return final_response
##################################################################################################
# ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¥¸ ê²€ìƒ‰
if user_input := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.write(user_input)
        
    # ìŒì‹ì  ê²€ìƒ‰ ë° ê²°ê³¼ ë°˜í™˜
    response = generate_response(user_input)
        
    # ì±—ë´‡ ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": response})

    # ì±—ë´‡ ì‘ë‹µ ì¶œë ¥
    with st.chat_message("assistant"):
        st.write(response)

# ì´ì „ ë©”ì‹œì§€ ì¶œë ¥ (ì„¸ì…˜ ìƒíƒœ ìœ ì§€)
for message in st.session_state.messages:  
    with st.chat_message(message["role"]):
        st.write(message["content"])    
    
